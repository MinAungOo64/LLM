{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4f7f86-11fc-4203-93e1-b12556f3ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\steve\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db686918-97ec-443c-b7fb-1fe44649c627",
   "metadata": {},
   "source": [
    "Tokenizing  \n",
    "1. Splits inputs into words, subwords/symbols called tokens\n",
    "2. map each token to integer\n",
    "3. add additional inputs useful to model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a8631-30b6-41d4-9338-1a2a8ae914f9",
   "metadata": {},
   "source": [
    "Checkpoints are saved versions of a model's weights and configurations at different training stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82caec34-0b4d-442b-bd5b-9675248fbffa",
   "metadata": {},
   "source": [
    "AutoTokenizer.from_pretrained extracts the tokenizer used during training of said model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127824e3-57b6-44ac-9203-3f0b458c48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687a643-6017-43b2-a7f1-ae84d8dbf628",
   "metadata": {},
   "source": [
    "Once we have the tokenizer, we can directly pass our sentences to it and we‚Äôll get back a dictionary that‚Äôs ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22b20f-e680-4413-8404-f1f0582b55d0",
   "metadata": {},
   "source": [
    "The output itself is a dictionary containing two keys, input_ids and attention_mask. input_ids contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11184319-9022-460e-9745-1b66578f7e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\") # pt specifies which type of tensor (pytorch, tensorflow or numpy)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f50fae-d08e-40a8-b18a-60046d375c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs[\"input_ids\"])  # Should return <class 'torch.Tensor'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b39873-7613-49ca-8c1c-eeee988c56eb",
   "metadata": {},
   "source": [
    "We can download our pretrained model the same way we did with our tokenizer. ü§ó Transformers provides an AutoModel class which also has a from_pretrained() method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b950e-1405-4ab0-8855-b20a8f356904",
   "metadata": {},
   "source": [
    "This model only outputs hidden states, not predictions like \"positive\" or \"negative.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2844d9-2981-48a0-acbf-57484cbdeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint) # AutoModel loads the base model (no classification, translation, etc.)\n",
    "# from_pretrained(checkpoint) downloads the model weights (or loads them from cache if already downloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484df69e-f8d9-487f-94a5-dea8d965dcdf",
   "metadata": {},
   "source": [
    "What Are Hidden States?  \n",
    "Hidden states are high-dimensional vectors (numerical representations) that capture the contextual meaning of the input text.  \n",
    "Each word in a sentence is converted into a vector.  \n",
    "These vectors are refined as they pass through multiple Transformer layers.  \n",
    "The final hidden states are useful for tasks like classification, translation, and question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb628f-4fc5-4b2f-be14-72c3fe92f3ad",
   "metadata": {},
   "source": [
    "A Transformer model without a head produces only hidden states. But for tasks like classification, we need a head‚Äîa small neural network that converts these hidden states into useful outputs.\n",
    "\n",
    "Example:  \n",
    "Sentiment Analysis ‚Üí A classification head outputs labels like positive or negative.  \n",
    "Text Generation ‚Üí A decoder head generates words.  \n",
    "Named Entity Recognition (NER) ‚Üí A token classification head labels words.  \n",
    "\n",
    "üí° For distilbert-base-uncased-finetuned-sst-2-english, the model is meant for classification, so we should use AutoModelForSequenceClassification instead of AutoModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5beb1-ecd7-46f5-a50f-f78f805cbc66",
   "metadata": {},
   "source": [
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "Batch size: The number of sequences processed at a time (2 sentences were passed in our example).  \n",
    "Sequence length: The length of the numerical representation of the sequence (16 elements in input vectors in our example).  \n",
    "Hidden size: The vector dimension of each model input.  \n",
    "It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faaaae8d-7cf3-45a6-b223-d80851b5ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e5442-d543-4ab9-b6d5-df1a9b9b96c3",
   "metadata": {},
   "source": [
    "Note that the outputs of ü§ó Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), or even by index if you know exactly where the thing you are looking for is (outputs[0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ad6b3c-7ee9-4882-ba3e-c192c0302a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
       "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
       "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
       "         ...,\n",
       "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
       "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
       "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
       "\n",
       "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
       "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
       "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
       "         ...,\n",
       "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
       "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
       "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8b557-1d39-497f-aa44-611a09d7b221",
   "metadata": {},
   "source": [
    "There are many different architectures available in ü§ó Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:\n",
    "\n",
    "*Model (retrieve the hidden states)  \n",
    "*ForCausalLM  \n",
    "*ForMaskedLM  \n",
    "*ForMultipleChoice  \n",
    "*ForQuestionAnswering  \n",
    "*ForSequenceClassification  \n",
    "*ForTokenClassification  \n",
    "and others ü§ó  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38fc58c3-de47-47f0-9701-57eb25e92ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint) # AutoModelForSequenceClassification for sentiment analysis\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0061055a-33d4-4631-95bb-da5ac1a161b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits # 2 labels for 2 sentences, raw logit values for each label for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70321a94-ee3a-47fc-9c38-b6e1c6be435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) # pass logits through sofmax for prob\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c5b4b-de48-452f-a116-988d0c0146bb",
   "metadata": {},
   "source": [
    "First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598  \n",
    "Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c63c272-ad15-482f-94d2-89bb9653d79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label # negative in index 0, postive in index 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373e64c-7419-4c94-81d8-aa4471c09621",
   "metadata": {},
   "source": [
    "AutoModel is wrapper that auotmatically detects the model of the chekpoint provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b09b38-057e-45f0-b4a9-73bc8a77e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")  # Loads a BERT model\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")  # Loads a RoBERTa model\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")  # Loads a DistilBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a961cc-4914-4290-bbdb-4f5447bce7a8",
   "metadata": {},
   "source": [
    "We can manually specify what model to load, the code below only loads checkpoints with Bert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b884fa3b-7e3f-48a2-b9a2-e044ef36186d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93e65e372954690a36cd69e36361d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\steve\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256ea5a747684d12bbc31ad8b9dfd25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234afbd-3489-4dc8-a947-bad1885ae1fe",
   "metadata": {},
   "source": [
    "Creating a transformer from scratch, all weights are randomly initialised, resource and time intensive to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8d7ef46-c018-47ac-a8a8-b87f5b8d66c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "# Building the model from the config\n",
    "model = BertModel(config)\n",
    "# Model is randomly initialized!\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6514ff-ab72-4e8e-84b1-71d1b7e4773d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b4d67c4-7a1b-4318-90b5-3c871c518f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7592,  999,  102],\n",
       "        [ 101, 4658, 1012,  102],\n",
       "        [ 101, 3835,  999,  102]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "# Tokenisers converts the sequences into input_ids\n",
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n",
    "\n",
    "import torch\n",
    "model_inputs = torch.tensor(encoded_sequences) # convert input ids into tensors\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9850e121-bb33-41e3-b269-a9df433a171b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.4496e-01,  4.8276e-01,  2.7797e-01,  ..., -5.4032e-02,\n",
       "           3.9394e-01, -9.4770e-02],\n",
       "         [ 2.4943e-01, -4.4093e-01,  8.1772e-01,  ..., -3.1917e-01,\n",
       "           2.2992e-01, -4.1172e-02],\n",
       "         [ 1.3668e-01,  2.2518e-01,  1.4502e-01,  ..., -4.6915e-02,\n",
       "           2.8224e-01,  7.5566e-02],\n",
       "         [ 1.1789e+00,  1.6738e-01, -1.8187e-01,  ...,  2.4671e-01,\n",
       "           1.0441e+00, -6.1966e-03]],\n",
       "\n",
       "        [[ 3.6436e-01,  3.2465e-02,  2.0258e-01,  ...,  6.0111e-02,\n",
       "           3.2451e-01, -2.0996e-02],\n",
       "         [ 7.1866e-01, -4.8725e-01,  5.1740e-01,  ..., -4.4012e-01,\n",
       "           1.4553e-01, -3.7545e-02],\n",
       "         [ 3.3223e-01, -2.3271e-01,  9.4876e-02,  ..., -2.5268e-01,\n",
       "           3.2172e-01,  8.1128e-04],\n",
       "         [ 1.2523e+00,  3.5754e-01, -5.1320e-02,  ..., -3.7840e-01,\n",
       "           1.0526e+00, -5.6255e-01]],\n",
       "\n",
       "        [[ 2.4042e-01,  1.4718e-01,  1.2110e-01,  ...,  7.6061e-02,\n",
       "           3.3564e-01,  2.8262e-01],\n",
       "         [ 6.5701e-01, -3.2787e-01,  2.4968e-01,  ..., -2.5920e-01,\n",
       "           2.0175e-01,  3.3275e-01],\n",
       "         [ 2.0160e-01,  1.5783e-01,  9.8972e-03,  ..., -3.8850e-01,\n",
       "           4.1308e-01,  3.9732e-01],\n",
       "         [ 1.0175e+00,  6.4387e-01, -7.8147e-01,  ..., -4.2109e-01,\n",
       "           1.0925e+00, -4.8456e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6856,  0.5262,  1.0000,  ...,  1.0000, -0.6112,  0.9971],\n",
       "        [-0.6055,  0.4997,  0.9998,  ...,  0.9999, -0.6753,  0.9769],\n",
       "        [-0.7702,  0.5447,  0.9999,  ...,  1.0000, -0.4655,  0.9894]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(model_inputs)\n",
    "output # outputs hidden states, pass into head for different tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32db620-f586-4720-9779-de24fe61c785",
   "metadata": {},
   "source": [
    "Vocabulary is defined by the total number of independent tokens that we have in our corpus.  \n",
    "['Jim', 'Henson', 'was', 'a', 'puppeteer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3548a89-6d69-45e2-81d1-11cc3861be21",
   "metadata": {},
   "source": [
    "There are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large ‚Äúvocabularies,‚Äù where a vocabulary is defined by the total number of independent tokens that we have in our corpus.\n",
    "\n",
    "Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.\n",
    "\n",
    "If we want to completely cover a language with a word-based tokenizer, we‚Äôll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we‚Äôd need to keep track of that many IDs. Furthermore, words like ‚Äúdog‚Äù are represented differently from words like ‚Äúdogs‚Äù, and the model will initially have no way of knowing that ‚Äúdog‚Äù and ‚Äúdogs‚Äù are similar: it will identify the two words as unrelated. The same applies to other similar words, like ‚Äúrun‚Äù and ‚Äúrunning‚Äù, which the model will not see as being similar initially.\n",
    "\n",
    "Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the ‚Äúunknown‚Äù token, often represented as ‚Äù[UNK]‚Äù or ‚Äù<unk>‚Äù. It‚Äôs generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn‚Äôt able to retrieve a sensible representation of a word and you‚Äôre losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token.\n",
    "\n",
    "One way to reduce the amount of unknown tokens is to go one level deeper, using a character-based tokenizer."
   ]
  },
  {
   "attachments": {
    "ad70c387-0295-4c8b-99a8-4dc37b106720.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "e343b98f-256c-4352-b317-c2508a89a03a",
   "metadata": {},
   "source": [
    "Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
    "\n",
    "The vocabulary is much smaller.\n",
    "There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
    "But here too some questions arise concerning spaces and punctuation:\n",
    "![image.png](attachment:ad70c387-0295-4c8b-99a8-4dc37b106720.png)\n",
    "An example of character-based tokenization.\n",
    "This approach isn‚Äôt perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it‚Äôs less meaningful: each character doesn‚Äôt mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language.\n",
    "\n",
    "Another thing to consider is that we‚Äôll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters.\n",
    "\n",
    "To get the best of both worlds, we can use a third technique that combines the two approaches: subword tokenization."
   ]
  },
  {
   "attachments": {
    "cb010306-2473-4a77-9f52-b21719964a8d.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "fd08256e-0192-4705-9d26-f9e2cfd5ea1e",
   "metadata": {},
   "source": [
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
    "\n",
    "For instance, ‚Äúannoyingly‚Äù might be considered a rare word and could be decomposed into ‚Äúannoying‚Äù and ‚Äúly‚Äù. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of ‚Äúannoyingly‚Äù is kept by the composite meaning of ‚Äúannoying‚Äù and ‚Äúly‚Äù.\n",
    "\n",
    "Here is an example showing how a subword tokenization algorithm would tokenize the sequence ‚ÄúLet‚Äôs do tokenization!‚Äú:\n",
    "![image.png](attachment:cb010306-2473-4a77-9f52-b21719964a8d.png)\n",
    "\n",
    "A subword tokenization algorithm.\n",
    "These subwords end up providing a lot of semantic meaning: for instance, in the example above ‚Äútokenization‚Äù was split into ‚Äútoken‚Äù and ‚Äúization‚Äù, two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.\n",
    "\n",
    "This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64872998-0f1a-4fa8-a70b-292aecbc30d7",
   "metadata": {},
   "source": [
    "Unsurprisingly, there are many more techniques out there. To name a few:  \n",
    "Byte-level BPE, as used in GPT-2  \n",
    "WordPiece, as used in BERT  \n",
    "SentencePiece or Unigram, as used in several multilingual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fed9cda7-0983-429b-bc26-143c02ce37ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "935987eb-63bb-4a03-b50c-6d901cc6cbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence) # use .tokenize to output each token in sub-words\n",
    "\n",
    "print(tokens) # sub-word tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15a05eca-36c4-42f4-8ddb-844129f4a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens) # input tokens and output input_ids\n",
    "print(ids)\n",
    "tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bc7e4f6-acee-42e3-98b4-ade0e1721bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '‚Äô', 've', 'been', 'waiting', 'for', 'a', 'Hu', '##gging', '##F', '##ace', 'course', 'my', 'whole', 'life', '.', 'I', 'hate', 'this', 'so', 'much', '!']\n",
      "[146, 787, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119, 146, 4819, 1142, 1177, 1277, 106]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 146, 787, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119, 102], [101, 146, 4819, 1142, 1177, 1277, 106, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = [\"I‚Äôve been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    "tokens = tokenizer.tokenize(sequence) # tokenize treats list as one string\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "tokenizer(sequence) # fast way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b6979-b5bf-4b6a-bc75-7d9bb6b7cd9f",
   "metadata": {},
   "source": [
    "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode() method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a20e8471-7ff3-421f-a9cd-460482300c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Using a transformer network is simple [SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13fabc6-0698-4fb1-a572-d21aa3b7130e",
   "metadata": {},
   "source": [
    "Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization).\n",
    "\n",
    "By now you should understand the atomic operations a tokenizer can handle: tokenization, conversion to IDs, and converting IDs back to a string. However, we‚Äôve just scraped the tip of the iceberg. In the following section, we‚Äôll take our approach to its limits and take a look at how to overcome them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a16978-8325-487f-9b36-2ff2c7c7dc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "079d7c2e-e8ba-4c1c-bbe8-2e45fdb5428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "         2026,  2878,  2166,  1012])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "print(input_ids)\n",
    "# This line will fail. ids should be in a list like this [ids], this is a batch\n",
    "# model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23f94817-7131-4e3b-8992-f7a920a4c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2ef090b-2036-45fa-a90f-53493a0fa06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b429e5-959c-4561-87d5-aea39d5eae8d",
   "metadata": {},
   "source": [
    "Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence:  \n",
    "batched_ids = [ids, ids]  \n",
    "This is a batch of two identical sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3245f7fb-c744-4b50-84bb-7fe1f513f777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012],\n",
      "        [ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789],\n",
      "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [ids, ids]\n",
    "input_ids = torch.tensor(batched_ids)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56466d-435f-425d-8114-c1e8b8efc6bf",
   "metadata": {},
   "source": [
    "Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. There‚Äôs a second issue, though. When you‚Äôre trying to batch together two (or more) sentences, they might be of different lengths. If you‚Äôve ever worked with tensors before, you know that they need to be of rectangular shape, so you won‚Äôt be able to convert the list of input IDs into a tensor directly. To work around this problem, we usually pad the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3dcc34-988a-4269-a093-d8861fa0f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list of lists cannot be converted to a tensor, diff dimensions\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66e689-ce9f-4584-a54a-42cc5cf177d0",
   "metadata": {},
   "source": [
    "In order to work around this, we‚Äôll use padding to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values. For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. In our example, the resulting tensor looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21df0d5c-b1fd-4b90-89c2-4c102cca3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48313a-d341-4ad5-bda6-c5365881ef63",
   "metadata": {},
   "source": [
    "The padding token ID can be found in tokenizer.pad_token_id. Let‚Äôs use it and send our two sentences through the model individually and batched together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bcd8b9c9-a6d7-4616-991d-931a7b987538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits) # logit values diff from logit of batched_ids\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f09b72-aeae-4778-8a19-5489a05dbce8",
   "metadata": {},
   "source": [
    "There‚Äôs something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we‚Äôve got completely different values!\n",
    "\n",
    "This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e186c83-4bb0-49b5-93d4-86803056f772",
   "metadata": {},
   "source": [
    "Attention masks  \n",
    "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
    "\n",
    "Let‚Äôs complete the previous example with an attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17475530-5840-4e92-92f1-f2694daa0cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)) # attention_mask also needs to be tensored\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits) # with attention masks, logit values should be the same\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197ad7c-9161-4ee6-9d9f-9003161125d2",
   "metadata": {},
   "source": [
    "Exercise: \n",
    "‚ÄúI‚Äôve been waiting for a HuggingFace course my whole life.‚Äù and ‚ÄúI hate this so much!‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e968fe6-1a85-4b75-86bb-9f3e9d38fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5720,  2.6852]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5720,  2.6852],\n",
      "        [ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "sequence1 = \"I‚Äôve been waiting for a HuggingFace course my whole life.\"\n",
    "sequence2 = \"I hate this so much!\"\n",
    "sequence1_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence1))]\n",
    "sequence2_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence2))]\n",
    "\n",
    "n = len(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence1)))-len(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence2)))\n",
    "sequence2_ids_paddings = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence2)) + [tokenizer.pad_token_id] * n\n",
    "batched_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence1)), sequence2_ids_paddings]\n",
    "attention_mask = [\n",
    "    [1] * len(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence1))),\n",
    "    [1] * len(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence2))) + [0] * n,\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))) # attention_mask also needs to be tensored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a2979-a2b8-4191-aed2-c06c8618b40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f659d055-d4a4-4871-a5d9-1744c649ea72",
   "metadata": {},
   "source": [
    "Longer sequences  \n",
    "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
    "\n",
    "Use a model with a longer supported sequence length.  \n",
    "Truncate your sequences.  \n",
    "Models have different supported sequence lengths, and some specialize in handling very long sequences. Longformer is one example, and another is LED.   If you‚Äôre working on a task that requires very long sequences, we recommend you take a look at those models.\n",
    "\n",
    "Otherwise, we recommend you truncate your sequences by specifying the max_sequence_length parameter:  \n",
    "sequence = sequence[:max_sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07fabedc-1009-435f-a898-088afa733adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Single Sequence\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs)\n",
    "\n",
    "# Multiple sequences\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "model_inputs = tokenizer(sequences)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3bf44-405c-4fee-89bd-9eaea6c69b65",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2d45b8a-ad7d-4674-914f-11dd34e81bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4e3b5-a998-4ca9-9313-4cc1b8d27d03",
   "metadata": {},
   "source": [
    "Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c5ae9-1252-4f19-945e-bd5e1292314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff2727-3da7-4b61-897c-3148de99a9ba",
   "metadata": {},
   "source": [
    "The tokenizer object can handle the conversion to specific framework tensors, which can then be directly sent to the model. For example, in the following code sample we are prompting the tokenizer to return tensors from the different frameworks ‚Äî \"pt\" returns PyTorch tensors, \"tf\" returns TensorFlow tensors, and \"np\" returns NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f1c03f5a-951d-47e0-8683-2a92ab133675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}\n",
      "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a33ff-5cd2-46dc-a0c9-afaf8f17f4b1",
   "metadata": {},
   "source": [
    "Special tokens 101 and 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec915bc4-13d5-46c5-9b44-bcef3dcb1a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "94d872cb-d05e-4f2b-be12-9691f628ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9567651-2367-4ae6-b529-b9f60cc27daf",
   "metadata": {},
   "source": [
    "The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don‚Äôt add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "42520313-353d-4cca-b475-9da8a1e94048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**model_inputs)\n",
    "'''\n",
    "In Python, when you pass ** before a dictionary, it unpacks the dictionary and passes each key-value pair as separate keyword arguments to the function. \n",
    "In this case, model(**model_inputs) unpacks the model_inputs dictionary \n",
    "and passes each of its keys (like 'input_ids', 'attention_mask', etc.) as keyword arguments to the model.\n",
    "output = model(input_ids=model_inputs['input_ids'], attention_mask=model_inputs['attention_mask'])\n",
    "'''\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301e1a8-8c0a-41ed-854a-132e76248993",
   "metadata": {},
   "source": [
    "Summary:  \n",
    "sequence -> tokens -> [token_ids + padding] -> tensors + attention mask -> hidden state 3 dims (sequence length, batch size, hidden size) -> output based on head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3f180-b7f8-44ec-b055-c64ea6112bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
